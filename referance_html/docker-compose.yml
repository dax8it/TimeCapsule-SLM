version: '3.8'

services:
  # TimeCapsule-SLM Web Application
  timecapsule-slm:
    build: .
    container_name: timecapsule-slm
    ports:
      - "3000:80"
    networks:
      - timecapsule-network
    restart: unless-stopped
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.timecapsule.rule=Host(`localhost`)"
      - "traefik.http.services.timecapsule.loadbalancer.server.port=80"
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Optional: Ollama AI Service for Local LLM Support
  ollama:
    image: ollama/ollama:latest
    container_name: timecapsule-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - timecapsule-network
    restart: unless-stopped
    environment:
      - OLLAMA_ORIGINS=http://localhost:3000,http://timecapsule-slm
      - OLLAMA_HOST=0.0.0.0:11434
    profiles:
      - ai-enabled
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/version"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # Optional: LM Studio Alternative (when using external LM Studio)
  # Uncomment if you want to use a separate LM Studio container
  # lm-studio:
  #   image: lmstudio/lm-studio:latest
  #   container_name: timecapsule-lmstudio
  #   ports:
  #     - "1234:1234"
  #   volumes:
  #     - lmstudio_data:/app/models
  #   networks:
  #     - timecapsule-network
  #   restart: unless-stopped
  #   profiles:
  #     - lmstudio-enabled

volumes:
  ollama_data:
    driver: local
    labels:
      - "description=Ollama model storage for TimeCapsule-SLM"
  
  # lmstudio_data:
  #   driver: local
  #   labels:
  #     - "description=LM Studio model storage for TimeCapsule-SLM"

networks:
  timecapsule-network:
    driver: bridge
    labels:
      - "description=Internal network for TimeCapsule-SLM services" 